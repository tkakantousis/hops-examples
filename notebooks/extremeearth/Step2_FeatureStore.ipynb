{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 1: Create Feature Groups and Train/Eval datasets\n",
    "This notebook will perform the following operations:\n",
    "- Read the pre-processed data from a HopsFS dataset into a PySpark dataframe \n",
    "- Create and Feature Group \"iceberg\"\n",
    "- Create a training and test dataset with the Feature Store API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "from hops import featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train_preprocessed_all.json\n",
      "train_preprocessed_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/train_preprocessed.json\n",
      "test_preprocessed_ds_path: hdfs://127.0.0.1:8020/Projects/ExtremeEarth/eodata/test_preprocessed.json"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'eodata'\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER,'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "train_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed.json')\n",
    "test_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'test_preprocessed.json')\n",
    "\n",
    "print(\"train_ds_path:\", train_ds_path)\n",
    "print(\"train_preprocessed_all_ds_path:\", train_preprocessed_all_ds_path)\n",
    "print(\"train_preprocessed_ds_path:\", train_preprocessed_ds_path)\n",
    "print(\"test_preprocessed_ds_path:\", test_preprocessed_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw train with spark and insert into feature store\n",
    "train_preprocessed_all_df = spark.read.format('json').load(train_preprocessed_all_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- band_1: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_2: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_avg: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inc_angle: string (nullable = true)\n",
      " |-- is_iceberg: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|              band_1|              band_2|            band_avg|      id|inc_angle|is_iceberg|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|[-27.878361, -27....|[-27.154118, -29....|[-27.5162395, -28...|dfd5f913|  43.9239|         0|\n",
      "|[-12.242375, -14....|[-31.506321, -27....|[-21.874348, -21....|e25388fd|  38.1562|         0|\n",
      "|[-24.603676, -24....|[-24.870956, -24....|[-24.737316, -24....|58b2aaa0|  45.2859|         1|\n",
      "|[-22.454607, -23....|[-27.889421, -27....|[-25.172014, -25....|4cfc3a18|  43.8306|         0|\n",
      "|[-26.006956, -23....|[-27.206915, -30....|[-26.6069355, -26...|271f93f4|  35.6256|         0|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save features to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering feature metadata...\n",
      "Registering feature metadata... [COMPLETE]\n",
      "Writing feature data to offline feature group (Hive)...\n",
      "Running sql: use extremeearth_featurestore against offline feature store\n",
      "Writing feature data to offline feature group (Hive)... [COMPLETE]\n",
      "Feature group created successfully"
     ]
    }
   ],
   "source": [
    "#read raw train with spark and insert into feature store\n",
    "train_preprocessed_all_df = spark.read.format('json').load(train_preprocessed_all_ds_path)\n",
    "\n",
    "featurestore.create_featuregroup(\n",
    "    train_preprocessed_all_df,\n",
    "    \"iceberg\",\n",
    "    description=\"Training dataset in Feature Store for iceberg classification\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "Now that preprocessing is done, let's split the feature data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use extremeearth_featurestore against offline feature store\n",
      "SQL string for the query created successfully\n",
      "Running sql: SELECT * FROM iceberg_1 against offline feature store"
     ]
    }
   ],
   "source": [
    "RAND_SEED = 42\n",
    "TRAIN_SIZE = 0.8\n",
    "icebergs_fg = featurestore.get_featuregroup(\"iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604"
     ]
    }
   ],
   "source": [
    "icebergs_fg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read feature group data, split into train/test and export in tfrecords\n",
    "icebergs_train_df, icebergs_test_df = icebergs_fg.randomSplit([TRAIN_SIZE, 1-TRAIN_SIZE], RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 1284 records"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset contains {} records\".format(icebergs_train_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset contains 320 records"
     ]
    }
   ],
   "source": [
    "print(\"Testing dataset contains {} records\".format(icebergs_test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write feature frame, write_mode: overwrite\n",
      "Training Dataset created successfully"
     ]
    }
   ],
   "source": [
    "# create a traiing dataset of TFRecord\n",
    "featurestore.create_training_dataset(\n",
    "    icebergs_train_df, \"train_tfrecords_iceberg_classification_dataset\",\n",
    "    data_format = \"tfrecords\",\n",
    "    descriptive_statistics = False,\n",
    "    feature_correlation = False,\n",
    "    feature_histograms = False,\n",
    "    cluster_analysis = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write feature frame, write_mode: overwrite\n",
      "Training Dataset created successfully"
     ]
    }
   ],
   "source": [
    "# create a test dataset of TFRecord\n",
    "featurestore.create_training_dataset(\n",
    "    icebergs_test_df, \"test_tfrecords_iceberg_classification_dataset\",\n",
    "    data_format = \"tfrecords\",\n",
    "    descriptive_statistics = False,\n",
    "    feature_correlation = False,\n",
    "    feature_histograms = False,\n",
    "    cluster_analysis = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}