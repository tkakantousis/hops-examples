{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Model Training in distributed training\n",
    "The following code includes demonstration for:\n",
    "- get data from ``feature store``\n",
    "- training with ``TFRecord``\n",
    "- distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>72</td><td>application_1574692443370_0076</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-184.eu-north-1.compute.internal:8088/proxy/application_1574692443370_0076/\">Link</a></td><td><a target=\"_blank\" href=\"http://localhost:8042/node/containerlogs/container_e03_1574692443370_0076_01_000001/ExtremeEarth__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Version of TensorFlow is 1.14.0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Version of TensorFlow is {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import featurestore\n",
    "from hops import experiment\n",
    "from hops import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_train():\n",
    "    tfrecord_path=\"train_tfrecords_iceberg_classification_dataset\"\n",
    "    name_list=[\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    dataset_dir = featurestore.get_training_dataset_path(tfrecord_path)\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    # 'tf_record_schema' is needed because we need to parse a single example from all the TFRecords we have\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(tfrecord_path)\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = tf.stack([example[name_list[0]], example[name_list[1]], example[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(example[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_test():\n",
    "    tfrecord_path=\"test_tfrecords_iceberg_classification_dataset\"\n",
    "    name_list=[\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    dataset_dir = featurestore.get_training_dataset_path(tfrecord_path)\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    # 'tf_record_schema' is needed because we need to parse a single example from all the TFRecords we have\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(tfrecord_path)\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = tf.stack([example[name_list[0]], example[name_list[1]], example[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(example[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Conv Layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(learning_rate):\n",
    "    \"\"\"\n",
    "    Defines the training loop:\n",
    "    \n",
    "    1. Get Model\n",
    "    2. Define custom metrics\n",
    "    3. Compile Model\n",
    "    4. Convert Keras model to TF Estimator\n",
    "    5. Fit model on train dataset\n",
    "    6. Evaluate model on validation dataset\n",
    "    7. Save validation results to HopsFS\n",
    "    8. Export trained model for serving\n",
    "    \"\"\"\n",
    "    # Tell Keras we are traning (in case it does different functionality between train/test time)\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "    # 1. Get model\n",
    "    print(\"Defning the model\")\n",
    "    model = create_model()\n",
    "    print(\"Defining the model complete\")\n",
    "    \n",
    "    \n",
    "    # 2. Compile the model\n",
    "    print(\"Compiling the model\")\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='binary_crossentropy',  \n",
    "                  metrics=['accuracy'])\n",
    "    print(\"Compiling the model complete\")\n",
    "    \n",
    "    # 3. Convert Keras model to TF Estimator\n",
    "    # Define DistributionStrategies and convert the Keras Model to an\n",
    "    # Estimator that utilizes these DistributionStrateges.\n",
    "    # Evaluator is a single worker, so using MirroredStrategy.\n",
    "    # Training is automatically distributed on all available GPUs when using MirroredStrategy\n",
    "    print(\"Convert keras model to a Tensorflow Estimator\")\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "#             train_distribute=tf.contrib.distribute.MirroredStrategy())\n",
    "            train_distribute=tf.distribute.experimental.MultiWorkerMirroredStrategy())\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(keras_model=model, \n",
    "               config=run_config, model_dir=tensorboard.logdir())\n",
    "    print(\"Keras model to estimator conversion complete\")\n",
    "    \n",
    "    \n",
    "    # 4. Fit model on training dataset\n",
    "    print(\"Starting training...\")\n",
    "    tf.estimator.train_and_evaluate(keras_estimator, train_spec=tf.estimator.TrainSpec(\n",
    "        input_fn=lambda: create_tf_dataset_train()),\n",
    "        eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=lambda: create_tf_dataset_test()))\n",
    "    print(\"Training complete\")\n",
    "    \n",
    "#     # 5. Evalute model on validation dataset\n",
    "#     print(\"Evaluating model on validation dataset\")\n",
    "#     eval_results = keras_estimator.evaluate(lambda: create_tf_dataset(VAL_DATASET, SHUFFLE_BUFFER_SIZE, BATCH_SIZE, NUM_EPOCHS))    \n",
    "#     val_top1acc = str(eval_results[\"accuracy\"])\n",
    "#     val_top3acc = str(eval_results[\"top3_acc\"])\n",
    "#     val_top5acc = str(eval_results[\"top5_acc\"])\n",
    "#     validation_results = {\n",
    "#         \"top1_acc\": val_top1acc,\n",
    "#         \"val_top3_acc\": val_top3acc,\n",
    "#         \"val_top5_acc\": val_top5acc\n",
    "#     }\n",
    "#     print(\"Evaluation complete\")\n",
    "    \n",
    "#     # 6. Save validation results to HopsFS\n",
    "#     print(\"Saving validation results to HopsFS..\")\n",
    "#     val_results_path = hdfs.project_path() + \"Resources/\" + VALIDATION_RESULTS_FILE \n",
    "#     hdfs.dump(json.dumps(validation_results), val_results_path)\n",
    "#     print(\"Saving validation results complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter for TFRecords\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "# Hyperparameter for learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "args_d = {}\n",
    "args_d[\"learning_rate\"] = [LEARNING_RATE]\n",
    "# Input shape of the model\n",
    "INPUT_SHAPE= (75, 75, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment"
     ]
    }
   ],
   "source": [
    "experiment_result_path = experiment.launch(\n",
    "    train_fn, \n",
    "    args_dict = args_d,\n",
    "    name='tinyimagenet_resnet_distributed_training',\n",
    "    description=\"Training TinyImageNet Using Distributed Training\",\n",
    "    local_logdir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}