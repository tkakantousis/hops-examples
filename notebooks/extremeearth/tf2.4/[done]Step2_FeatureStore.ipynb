{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Create Feature Groups and Train/Eval datasets\n",
    "This notebook will perform the following operations:\n",
    "- Read the pre-processed data from a HopsFS dataset into a PySpark dataframe \n",
    "- Create and Feature Group \"iceberg\"\n",
    "- Create a training and test dataset with the Feature Store API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>30</td><td>application_1617271333718_0139</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1617271333718_0139/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks-3.novalocal:8042/node/containerlogs/container_e03_1617271333718_0139_01_000001/ExtremeEarth__theo0000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "import hsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train_preprocessed_all.json\n",
      "train_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/train_preprocessed.json\n",
      "test_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/ExtremeEarth/eodata/test_preprocessed.json"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'eodata'\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER,'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "train_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed.json')\n",
    "test_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'test_preprocessed.json')\n",
    "\n",
    "print(\"train_ds_path:\", train_ds_path)\n",
    "print(\"train_preprocessed_all_ds_path:\", train_preprocessed_all_ds_path)\n",
    "print(\"train_preprocessed_ds_path:\", train_preprocessed_ds_path)\n",
    "print(\"test_preprocessed_ds_path:\", test_preprocessed_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw train with spark and insert into feature store\n",
    "train_preprocessed_all_df = spark.read.format('json').load(train_preprocessed_all_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- band_1: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_2: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_avg: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inc_angle: string (nullable = true)\n",
      " |-- is_iceberg: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|              band_1|              band_2|            band_avg|      id|inc_angle|is_iceberg|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|[-27.878361, -27....|[-27.154118, -29....|[-27.5162395, -28...|dfd5f913|  43.9239|         0|\n",
      "|[-12.242375, -14....|[-31.506321, -27....|[-21.874348, -21....|e25388fd|  38.1562|         0|\n",
      "|[-24.603676, -24....|[-24.870956, -24....|[-24.737316, -24....|58b2aaa0|  45.2859|         1|\n",
      "|[-22.454607, -23....|[-27.889421, -27....|[-25.172014, -25....|4cfc3a18|  43.8306|         0|\n",
      "|[-26.006956, -23....|[-27.206915, -30....|[-26.6069355, -26...|271f93f4|  35.6256|         0|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save features to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "icebergs_fg = fs.create_feature_group(\n",
    "    \"iceberg\",\n",
    "    time_travel_format=None,\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    description=\"Training dataset in Feature Store for iceberg classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fb2f7ea1910>\n",
      "VersionWarning: No version provided for creating feature group `iceberg`, incremented version to `1`."
     ]
    }
   ],
   "source": [
    "icebergs_fg.save(train_preprocessed_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "Now that preprocessing is done, let's split the feature data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 42\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604"
     ]
    }
   ],
   "source": [
    "icebergs_fg.read().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read feature group data, split into train/test and export in tfrecords\n",
    "icebergs_train_df, icebergs_test_df = icebergs_fg.read().randomSplit([TRAIN_SIZE, 1-TRAIN_SIZE], RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 1263 records"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset contains {} records\".format(icebergs_train_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset contains 341 records"
     ]
    }
   ],
   "source": [
    "print(\"Testing dataset contains {} records\".format(icebergs_test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset `train_tfrecords_iceberg_classification_dataset`, incremented version to `1`."
     ]
    }
   ],
   "source": [
    "# create a traiing dataset of TFRecord\n",
    "icebergs_train_td = fs.create_training_dataset(\n",
    "    \"train_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset `test_tfrecords_iceberg_classification_dataset`, incremented version to `1`."
     ]
    }
   ],
   "source": [
    "# create a traiing dataset of TFRecord\n",
    "icebergs_test_td = fs.create_training_dataset(\n",
    "    \"test_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}