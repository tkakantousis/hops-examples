{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Model Training with Single GPU\n",
    "The following code includes demonstration for:\n",
    "- get data (``TFRecord``) from ``feature store``\n",
    "- training with ``TFRecord`` on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version of TensorFlow is 1.14.0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Version of TensorFlow is {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import featurestore\n",
    "from hops import experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(tfrecord_path, name_list):\n",
    "    dataset_dir = featurestore.get_training_dataset_path(tfrecord_path)\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    # 'tf_record_schema' is needed because we need to parse a single example from all the TFRecords we have\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(tfrecord_path)\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = tf.stack([example[name_list[0]], example[name_list[1]], example[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(example[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Conv Layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn():\n",
    "    from hops import tensorboard\n",
    "    train_tfrecord_path = \"train_tfrecords_iceberg_classification_dataset\"\n",
    "    train_name_list = [\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    train_dataset = create_tf_dataset(train_tfrecord_path, train_name_list)\n",
    "    \n",
    "    test_tfrecord_path = \"test_tfrecords_iceberg_classification_dataset\"\n",
    "    test_name_list = [\"band_1\", \"band_2\", \"band_avg\", \"is_iceberg\"]\n",
    "    test_dataset = create_tf_dataset(test_tfrecord_path, test_name_list)\n",
    "    \n",
    "    \n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(LEARNING_RATE), loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard.logdir(), histogram_freq=0, write_graph=True, write_images=True)\n",
    "    callbacks = [tb_callback]\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(tensorboard.logdir() + '/checkpoint-{epoch}.h5',\n",
    "                    monitor='acc', verbose=0, save_best_only=True))\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "            train_distribute=tf.contrib.distribute.MirroredStrategy())\n",
    "\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "    keras_estimator = tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config, model_dir=tensorboard.logdir())\n",
    "    metrics = tf.estimator.train_and_evaluate(keras_estimator, \n",
    "                                              train_spec=tf.estimator.TrainSpec(input_fn=lambda:create_tf_dataset(train_tfrecord_path, train_name_list)), \n",
    "                                              eval_spec=tf.estimator.EvalSpec(input_fn=lambda:create_tf_dataset(train_tfrecord_path, train_name_list)))\n",
    "    \n",
    "    export_model(keras_estimator, 1)\n",
    "    return metrics[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(classifier, version):\n",
    "    \"\"\"\n",
    "    Exports trained model \n",
    "    \n",
    "    Args:\n",
    "        :classifier: the model to export\n",
    "        :version: version of the model to export\n",
    "    \"\"\"\n",
    "    from tensorflow_transform.tf_metadata import schema_utils\n",
    "    from hops import hdfs\n",
    "    import tensorflow_model_analysis as tfma\n",
    "    import tensorflow_metadata as tfm\n",
    "\n",
    "    def _serving_input_receiver_fn():\n",
    "        # key (e.g. 'examples') should be same with the inputKey when you \n",
    "        # buid the request for prediction\n",
    "        receiver_tensors = {\"conv2d_input\":tf.placeholder(dtype=tf.float32,shape=[1,75,75,3])}\n",
    "        return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\n",
    "   \n",
    "    from hops import serving\n",
    "    import os\n",
    "    local_export_dir = os.getcwd()\n",
    "    exported_path = classifier.export_savedmodel(local_export_dir, _serving_input_receiver_fn)\n",
    "    \n",
    "    exported_path = exported_path.decode(\"utf-8\")\n",
    "    serving.export(exported_path, \"icebergmodel\", version, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_model(classifier, version):\n",
    "#     \"\"\"\n",
    "#     Exports trained model \n",
    "    \n",
    "#     Args:\n",
    "#         :classifier: the model to export\n",
    "#         :version: version of the model to export\n",
    "#     \"\"\"\n",
    "#     from tensorflow_transform.tf_metadata import schema_utils\n",
    "#     from hops import hdfs\n",
    "#     import tensorflow_model_analysis as tfma\n",
    "#     import tensorflow_metadata as tfm\n",
    "\n",
    "#     def _serving_input_receiver_fn():\n",
    "#         # key (e.g. 'examples') should be same with the inputKey when you \n",
    "#         # buid the request for prediction\n",
    "#         receiver_tensors = {\"conv2d_input\":tf.placeholder(dtype=tf.float32,shape=[1,75,75,3])}\n",
    "#         return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\n",
    "#     def eval_input_receiver_fn():\n",
    "#         # key (e.g. 'examples') should be same with the inputKey when you \n",
    "#         # buid the request for prediction\n",
    "#         receiver_tensors = {\"examples\":tf.placeholder(dtype=tf.float32,shape=[1,75,75,3])}\n",
    "#         # Extract feature spec from the schema.\n",
    "#         hdfs.copy_to_local('hdfs:///Projects/ExtremeEarth/ExtremeEarth_Training_Datasets/train_tfrecords_iceberg_classification_dataset_1/tf_record_schema.txt', '')\n",
    "#         with open('tf_record_schema.txt','r') as f:\n",
    "#             schema = f.read()\n",
    "#             schema = tfm.proto.v0.schema_pb2.Schema().FromString(schema)\n",
    "# #         raw_feature_spec = schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "#         conv2d_input = tf.feature_column.numeric_column('conv2d_input')\n",
    "#         feature_spec =  tf.feature_column.make_parse_example_spec(conv2d_input)\n",
    "#         serialized_tf_example = tf.placeholder(dtype=tf.float32,shape=[1,75,75,3], name='input_example_tensor')\n",
    "#         features = tf.io.parse_example(receiver_tensors, feature_spec)\n",
    "\n",
    "#         # First we deserialize our examples using the raw schema.\n",
    "# #         features = tf.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "\n",
    "#         return tfma.export.EvalInputReceiver(receiver_tensors=receiver_tensors, features=features, labels=features['is_iceberg'])\n",
    "\n",
    "\n",
    "#     from hops import serving\n",
    "#     import os\n",
    "#     local_export_dir = os.getcwd()\n",
    "#     exported_path = classifier.export_savedmodel(local_export_dir, _serving_input_receiver_fn)\n",
    "    \n",
    "#     # Also export the EvalSavedModel\n",
    "#     tfma.export.export_eval_savedmodel(\n",
    "#     estimator=classifier, export_dir_base=\"hdfs:///Projects/ExtremeEarth/tfx/tfma\",\n",
    "#     eval_input_receiver_fn=eval_input_receiver_fn)\n",
    "\n",
    "#     exported_path = exported_path.decode(\"utf-8\")\n",
    "#     serving.export(exported_path, \"icebergmodel\", version, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter for TFRecords\n",
    "# NUM_EPOCHS = 150\n",
    "NUM_EPOCHS = 1 # as we are limited with CPU for demo\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "# Hyperparameter for learning rate\n",
    "LEARNING_RATE = 0.001\n",
    "# Input shape of the model\n",
    "INPUT_SHAPE= (75, 75, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/launcher.py\", line 185, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 27, in train_fn\n",
      "  File \"<stdin>\", line 48, in export_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/util.py\", line 173, in wrapped_fn\n",
      "    return fn(**kwargs_to_pass)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/eval_saved_model/export.py\", line 372, in export_eval_savedmodel\n",
      "    checkpoint_path=checkpoint_path)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 806, in experimental_export_all_saved_models\n",
      "    checkpoint_path=checkpoint_path, strip_default_attrs=True)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 850, in _export_all_saved_models\n",
      "    strip_default_attrs=strip_default_attrs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 922, in _add_meta_graph_for_mode\n",
      "    input_receiver = input_receiver_fn()\n",
      "  File \"<stdin>\", line 29, in eval_input_receiver_fn\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 1329, in numeric_column\n",
      "    fc_utils.assert_key_is_string(key)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/utils.py\", line 65, in assert_key_is_string\n",
      "    type(key), key))\n",
      "ValueError: key must be a string. Got: type <type 'list'>. Given key: ['conv2d_input'].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/launcher.py\", line 185, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 27, in train_fn\n",
      "  File \"<stdin>\", line 48, in export_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/util.py\", line 173, in wrapped_fn\n",
      "    return fn(**kwargs_to_pass)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/eval_saved_model/export.py\", line 372, in export_eval_savedmodel\n",
      "    checkpoint_path=checkpoint_path)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 806, in experimental_export_all_saved_models\n",
      "    checkpoint_path=checkpoint_path, strip_default_attrs=True)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 850, in _export_all_saved_models\n",
      "    strip_default_attrs=strip_default_attrs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 922, in _add_meta_graph_for_mode\n",
      "    input_receiver = input_receiver_fn()\n",
      "  File \"<stdin>\", line 29, in eval_input_receiver_fn\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 1329, in numeric_column\n",
      "    fc_utils.assert_key_is_string(key)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/utils.py\", line 65, in assert_key_is_string\n",
      "    type(key), key))\n",
      "ValueError: key must be a string. Got: type <type 'list'>. Given key: ['conv2d_input'].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/experiment.py\", line 225, in launch\n",
      "    retval, tensorboard_logdir, hp = launcher._launch(sc, map_fun, args_dict, local_logdir)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/launcher.py\", line 52, in _launch\n",
      "    nodeRDD.foreachPartition(_prepare_func(app_id, run_id, map_fun, args_dict, local_logdir))\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 806, in foreachPartition\n",
      "    self.mapPartitions(func).count()  # Force evaluation\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1055, in count\n",
      "    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1046, in sum\n",
      "    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 917, in fold\n",
      "    vals = self.mapPartitions(func).collect()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/srv/hops/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/launcher.py\", line 185, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 27, in train_fn\n",
      "  File \"<stdin>\", line 48, in export_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/util.py\", line 173, in wrapped_fn\n",
      "    return fn(**kwargs_to_pass)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/eval_saved_model/export.py\", line 372, in export_eval_savedmodel\n",
      "    checkpoint_path=checkpoint_path)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 806, in experimental_export_all_saved_models\n",
      "    checkpoint_path=checkpoint_path, strip_default_attrs=True)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 850, in _export_all_saved_models\n",
      "    strip_default_attrs=strip_default_attrs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 922, in _add_meta_graph_for_mode\n",
      "    input_receiver = input_receiver_fn()\n",
      "  File \"<stdin>\", line 29, in eval_input_receiver_fn\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 1329, in numeric_column\n",
      "    fc_utils.assert_key_is_string(key)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/utils.py\", line 65, in assert_key_is_string\n",
      "    type(key), key))\n",
      "ValueError: key must be a string. Got: type <type 'list'>. Given key: ['conv2d_input'].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/hops/launcher.py\", line 185, in _wrapper_fun\n",
      "    retval = map_fun()\n",
      "  File \"<stdin>\", line 27, in train_fn\n",
      "  File \"<stdin>\", line 48, in export_model\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/util.py\", line 173, in wrapped_fn\n",
      "    return fn(**kwargs_to_pass)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_model_analysis/eval_saved_model/export.py\", line 372, in export_eval_savedmodel\n",
      "    checkpoint_path=checkpoint_path)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 806, in experimental_export_all_saved_models\n",
      "    checkpoint_path=checkpoint_path, strip_default_attrs=True)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 850, in _export_all_saved_models\n",
      "    strip_default_attrs=strip_default_attrs)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 922, in _add_meta_graph_for_mode\n",
      "    input_receiver = input_receiver_fn()\n",
      "  File \"<stdin>\", line 29, in eval_input_receiver_fn\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py\", line 1329, in numeric_column\n",
      "    fc_utils.assert_key_is_string(key)\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/python27/lib/python2.7/site-packages/tensorflow/python/feature_column/utils.py\", line 65, in assert_key_is_string\n",
      "    type(key), key))\n",
      "ValueError: key must be a string. Got: type <type 'list'>. Given key: ['conv2d_input'].\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.launch(train_fn, name=\"Iceberg_classification_with_featurestore_and_TFRecords\", local_logdir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END of the Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}