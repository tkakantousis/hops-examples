{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Conversion from JPEGs to TFRecords with feature and labels. Notebook (1/4) in the End-to-End Scalable Deep Learning Pipeline on Hops.\n",
    "\n",
    "TinyImageNet dataset contains a training set of 100,000 images, a validation set of 10,000 images, and a test\n",
    "set of also 10,000 images. These images are sourced from 200 different classes of objects. The images are downscaled from the original ImageNet’s dataset size of 256x256 to 64x64. \n",
    "\n",
    "The two initial tasks before we can train a model on this dataset are:\n",
    "\n",
    "1. Group the images with their labels \n",
    "2. Convert the JPEGs into TFRecords\n",
    "\n",
    "When using large datasets, like ImageNet, dealing with JPEGs is not very efficient, nor compatible with all of the functionality in the Tensorflow framework. \n",
    "\n",
    "TFRecords is a binary format for representing features and labels in Tensoflow, using a binary format for a large dataset can have a huge impact on disk space and processing speed. In addition, TFRecords are easier to work with than working with the raw JPEGs. \n",
    "\n",
    "However, TFRecords is not very friendly to query and analyze, so we can also store the feature data as Hive tables in the feature store.\n",
    "\n",
    "![step1.png](./../images/step1.png)\n",
    "\n",
    "This notebook read JPEGs and labels from:\n",
    "\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/train\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/val\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/test\n",
    "\n",
    "and dataset metadata from:\n",
    "\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/words.txt\n",
    "- hdfs:///Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/val/val_annotations.txt\n",
    "\n",
    "The notebook outputs feature groups (Hive ORC tables):\n",
    "\n",
    "- train\n",
    "- test\n",
    "- val\n",
    "\n",
    "The notebook outputs the following feature store training datasets (TFRecord files):\n",
    "\n",
    "- train_dataset_tinyimagenet\n",
    "- test_dataset_tinyimagenet\n",
    "- val_dataset_tinyimagenet\n",
    "\n",
    "To run this notebook you must have installed matplotlib in addition to the libraries available in the base conda environment. Python 3.6 is recommended, this notebook have only been tested with that version and python 2.7 will be deprecated eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1372</td><td>application_1553861944920_0364</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop33:8088/proxy/application_1553861944920_0364/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop9:8042/node/containerlogs/container_e71_1553861944920_0364_01_000001/end_to_end__payberah\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pydoop.hdfs as py_hdfs\n",
    "from hops import hdfs\n",
    "import numpy as np\n",
    "from hops import featurestore\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame, Row\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = hdfs.project_path()\n",
    "SHARED_DATASET = \"hdfs://10.0.104.196:8020/Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/\"\n",
    "TRAIN_DIR = SHARED_DATASET + \"train\"\n",
    "TEST_DIR = SHARED_DATASET + \"test\"\n",
    "VAL_DIR = SHARED_DATASET + \"val/images/\"\n",
    "ID_TO_CLASS_FILE = SHARED_DATASET + \"/words.txt\"\n",
    "VAL_LABELS_FILE = SHARED_DATASET + \"val/val_annotations.txt\"\n",
    "FILE_PATTERN = \"*.JPEG\"\n",
    "SIZES_FILE = PROJECT_DIR + \"Resources/sizes.txt\"\n",
    "TRAIN_FEATUREGROUP = \"train_tinyimagenet\"\n",
    "TRAIN_FEATUREGROUP_VERSION = 1\n",
    "TEST_FEATUREGROUP = \"test_tinyimagenet\"\n",
    "TEST_FEATUREGROUP_VERSION = 1\n",
    "VAL_FEATUREGROUP = \"val_tinyimagenet\"\n",
    "VAL_FEATUREGROUP_VERSION = 1\n",
    "TRAIN_DATASET = \"train_dataset_tinyimagenet\"\n",
    "TEST_DATASET = \"test_dataset_tinyimagenet\"\n",
    "VAL_DATASET = \"val_dataset_tinyimagenet\"\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants in %%local environment as well, for plotting and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "PROJECT_DIR = hdfs.project_path()\n",
    "SHARED_DATASET = \"hdfs://10.0.104.196:8020/Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/\"\n",
    "TRAIN_DIR = SHARED_DATASET + \"train\"\n",
    "TEST_DIR = SHARED_DATASET + \"test\"\n",
    "VAL_DIR = SHARED_DATASET + \"val/images/\"\n",
    "ID_TO_CLASS_FILE = SHARED_DATASET + \"/words.txt\"\n",
    "VAL_LABELS_FILE = SHARED_DATASET + \"val/val_annotations.txt\"\n",
    "FILE_PATTERN = \"*.JPEG\"\n",
    "SIZES_FILE = PROJECT_DIR + \"Resources/sizes.txt\"\n",
    "TRAIN_FEATUREGROUP = \"train_tinyimagenet\"\n",
    "TRAIN_FEATUREGROUP_VERSION = 1\n",
    "TEST_FEATUREGROUP = \"test_tinyimagenet\"\n",
    "TEST_FEATUREGROUP_VERSION = 1\n",
    "VAL_FEATUREGROUP = \"val_tinyimagenet\"\n",
    "VAL_FEATUREGROUP_VERSION = 1\n",
    "TRAIN_DATASET = \"train_dataset_tinyimagenet\"\n",
    "TEST_DATASET = \"test_dataset_tinyimagenet\"\n",
    "VAL_DATASET = \"val_dataset_tinyimagenet\"\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We can inspect some sample images from the dataset. Since Tensorflow support HDFS as a data source, we can use Tensorflow for reading from HDFS and then plot the images with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "base_path = \"hdfs://10.0.104.196:8020/Projects/ImageNet_EndToEnd_MLPipeline/tiny-imagenet/tiny-imagenet-200/train\"\n",
    "sample_images = [\n",
    "    base_path + \"/n01443537/images/n01443537_327.JPEG\",\n",
    "    base_path + \"/n01698640/images/n01698640_381.JPEG\",\n",
    "    base_path + \"/n01882714/images/n01882714_300.JPEG\",\n",
    "    base_path + \"/n01774750/images/n01774750_24.JPEG\",\n",
    "    base_path + \"/n09332890/images/n09332890_300.JPEG\",\n",
    "    base_path + \"/n01443537/images/n01443537_328.JPEG\",\n",
    "    base_path + \"/n01770393/images/n01770393_101.JPEG\",\n",
    "    base_path + \"/n01784675/images/n01784675_101.JPEG\"\n",
    "]\n",
    "img_op_0 = tf.image.decode_jpeg(tf.read_file(sample_images[0]))\n",
    "img_op_1 = tf.image.decode_jpeg(tf.read_file(sample_images[1]))\n",
    "img_op_2 = tf.image.decode_jpeg(tf.read_file(sample_images[2]))\n",
    "img_op_3 = tf.image.decode_jpeg(tf.read_file(sample_images[3]))\n",
    "img_op_4 = tf.image.decode_jpeg(tf.read_file(sample_images[4]))\n",
    "img_op_5 = tf.image.decode_jpeg(tf.read_file(sample_images[5]))\n",
    "img_op_6 = tf.image.decode_jpeg(tf.read_file(sample_images[6]))\n",
    "img_op_7 = tf.image.decode_jpeg(tf.read_file(sample_images[7]))\n",
    "sample_images_parsed = []\n",
    "with tf.Session() as sess:\n",
    "    sample_images_parsed.append(img_op_0.eval())\n",
    "    sample_images_parsed.append(img_op_1.eval())\n",
    "    sample_images_parsed.append(img_op_2.eval())\n",
    "    sample_images_parsed.append(img_op_3.eval())\n",
    "    sample_images_parsed.append(img_op_4.eval())\n",
    "    sample_images_parsed.append(img_op_5.eval())\n",
    "    sample_images_parsed.append(img_op_6.eval())\n",
    "    sample_images_parsed.append(img_op_7.eval())\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,10)\n",
    "count = 0\n",
    "for img in sample_images_parsed:\n",
    "    count += 1\n",
    "    plt.subplot(4,4,count)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the class distribution in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# this will take 1-2 minutes\n",
    "number_of_examples_per_class = list(map(lambda d: len(hdfs.ls(d + \"/images/\")), hdfs.ls(TRAIN_DIR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the examples are uniformly distributed over the classes. In the training dataset there are 200 classes with 500 examples in each (total 100 000 images in the training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.hist(number_of_examples_per_class, bins='auto')\n",
    "plt.xlabel('Number of examples)')\n",
    "plt.ylabel('Number of classes')\n",
    "plt.title('Class distribution histogram')\n",
    "plt.savefig(\"class_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Metadata about the Dataset\n",
    "\n",
    "The dataset has some .txt files with annotation and other metadata that needs to be parsed. This metadata for example explains what the label for the image directories are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata():\n",
    "    \"\"\" \n",
    "    Parses the words.txt file into a map of label -> words and a list of ordered nids (index of nid = integer label).\n",
    "    Also parses the val_annotations.txt file into a map of (validation_file_name --> nid)\n",
    "    \"\"\"\n",
    "    # list all directories in the train set, the directory name is the \"nid\" and identifies the label\n",
    "    train_dirs = py_hdfs.ls(TRAIN_DIR)\n",
    "    \n",
    "    # remove the path except the nid\n",
    "    train_nid_list = list(map(lambda x: x.replace(TRAIN_DIR + \"/\", \"\"), train_dirs))\n",
    "    \n",
    "    # the number of nids equal the number of unique classes/labels\n",
    "    num_classes = len(train_nid_list)\n",
    "    \n",
    "    # read the words.txt file that contains lines of the form \"nid\\words\"\n",
    "    with py_hdfs.open(ID_TO_CLASS_FILE, 'r') as f:\n",
    "        file_lines = f.read().decode(\"utf-8\").split(\"\\n\")\n",
    "    label_to_word = {}\n",
    "    \n",
    "    for l in file_lines:\n",
    "        # parse each line\n",
    "        wnid, word = l.split('\\t')\n",
    "        if wnid in train_nid_list:\n",
    "            # convert the nids into integer labels by using the position in the index\n",
    "            label = train_nid_list.index(wnid)\n",
    "            word = str(label) + \": \" + word\n",
    "            # save the mapping of integer label --> words\n",
    "            label_to_word[label] = word\n",
    "    \n",
    "    # read the val_annotations.txt file that contains lines of the form: \n",
    "    # \"validation_image\\tnid\\tx_pos\\ty_pos\\tw_pos\\th_pos\"\n",
    "    with py_hdfs.open(VAL_LABELS_FILE, 'r') as f:\n",
    "        file_lines = f.read().decode(\"utf-8\").split(\"\\n\")\n",
    "    validation_file_to_nid = {}\n",
    "    for l in file_lines:\n",
    "        # parse each line\n",
    "        tokens = l.split('\\t')\n",
    "        #skip corrupted lines\n",
    "        if len(tokens) > 2:\n",
    "            validation_img = tokens[0]\n",
    "            wnid = tokens[1]\n",
    "            # we only care about classification in this tutorial, not localization \n",
    "            if wnid in train_nid_list:\n",
    "                validation_file_to_nid[validation_img] = wnid\n",
    "    \n",
    "    return train_nid_list, label_to_word, validation_file_to_nid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Training Data to the Feature Store as a Feature Group\n",
    "\n",
    "The training data is split into several directories, we need to join them together before we can put them in the featurestore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_shapes(row):\n",
    "    \"\"\"\n",
    "    Filter to remove images of wrong shape. \n",
    "    2% of the tinyimagenet images are not colored, while the rest are, this filter will simply drop\n",
    "    the non-colored images.\n",
    "    \n",
    "    Args:\n",
    "          :row: a row from a spark dataframe containing images\n",
    "    \n",
    "    Returns:\n",
    "           True if the image matches the expected dimensions, False otherwise\n",
    "          \n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = row.image\n",
    "        height = image.height\n",
    "        width = image.width\n",
    "        nChannels = image.nChannels\n",
    "        if height != HEIGHT or width != WIDTH or nChannels != CHANNELS:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_train_dirs_into_featurestore():\n",
    "    \"\"\"\n",
    "    Loops over all directories with training data and inserts them into the featurestore.\n",
    "    There are 500 images in each directory, the name of the directory indicates the label of the images.\n",
    "    \n",
    "    Inserts the training data into the featurestore as a feature group (Hive table) with schema:\n",
    "    \n",
    "    |image|label|\n",
    "    \"\"\"\n",
    "    # Make sure feature store metadata is up to date\n",
    "    featurestore.get_featurestore_metadata(update_cache=True)\n",
    "    # Parse TinyImageNet metadata file from HopsFS\n",
    "    nid_list, label_to_word, validation_file_to_nid = parse_metadata()\n",
    "    # Read the train dir from HopsFS into a spark image dataframe\n",
    "    train_dir = TRAIN_DIR + \"/*/images/\"\n",
    "    image_df = spark.read.format(\"image\").load(train_dir)\n",
    "    # Filter out images with wrong dimensions\n",
    "    image_df_filtered = image_df.rdd.filter(filter_shapes).toDF()\n",
    "    # Add the label to each image file (label can be inferred using the nid_list and the image file name)\n",
    "    def map_label(row):\n",
    "        Example = Row(\"image\", \"label\")\n",
    "        filename = row.image.origin\n",
    "        # Extract label from the file name and the nid_list\n",
    "        label = nid_list.index(re.sub(\"[^images]*$\", \"\", filename.replace(TRAIN_DIR, \"\").replace(\"/\", \"\")).replace(\"images\",\"\"))\n",
    "        return Example(row.image, label)\n",
    "    features_df = image_df_filtered.rdd.map(map_label).toDF()\n",
    "    featurestore.create_featuregroup(features_df, TRAIN_FEATUREGROUP, \n",
    "                                     description=\"train data for image classification with TinyImageNet, contains labelled train images\",\n",
    "                                     feature_correlation=False, \n",
    "                                     cluster_analysis=False,\n",
    "                                    feature_histograms=False,\n",
    "                                    descriptive_statistics=True,\n",
    "                                    featuregroup_version=TRAIN_FEATUREGROUP_VERSION\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Testing Data to the Feature Store as a Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_test_dir_into_featurestore():\n",
    "    \"\"\"\n",
    "    Reads a single directory of images from HopsFS and inserts it into \n",
    "    the featurestore as a feature group (Hive table) with schema:\n",
    "    \n",
    "    |image|\n",
    "    \"\"\"\n",
    "    test_dir = TEST_DIR + \"/images/\"\n",
    "    # Read the train dir from HopsFS into a spark image dataframe\n",
    "    image_df = spark.read.format(\"image\").load(test_dir)\n",
    "    # Filter out images with wrong dimensions\n",
    "    image_df_filtered = image_df.rdd.filter(filter_shapes).toDF()\n",
    "    # Insert into feature store as a feature group\n",
    "    featurestore.create_featuregroup(image_df_filtered, TEST_FEATUREGROUP, \n",
    "                                     description = \"test data for image classification with TinyImageNet, contains unlabelled test images\",\n",
    "                                     feature_correlation=False, \n",
    "                                     cluster_analysis=False, feature_histograms=False,\n",
    "                                     descriptive_statistics=True, \n",
    "                                     featuregroup_version=TEST_FEATUREGROUP_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Validation Data to the Feature Store as a Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_val_dir_into_featurestore():\n",
    "    \"\"\"\n",
    "    Reads a single directory of images from HopsFS and inserts it into \n",
    "    the featurestore as a feature group (Hive table) with schema:\n",
    "    \n",
    "    |image|label|\n",
    "    \"\"\"\n",
    "    # Parse TinyImageNet metadata file from HopsFS\n",
    "    nid_list, label_to_word, validation_file_to_nid = parse_metadata()\n",
    "    # Read the train dir from HopsFS into a spark image dataframe\n",
    "    val_dir = VAL_DIR\n",
    "    image_df = spark.read.format(\"image\").load(val_dir)\n",
    "    # Filter out images with wrong dimensions\n",
    "    image_df_filtered = image_df.rdd.filter(filter_shapes).toDF()\n",
    "    # Add the label to each image file (label can be inferred using the nid_list and the image file name)\n",
    "    def map_label(row):\n",
    "        Example = Row(\"image\", \"label\")\n",
    "        filename = row.image.origin\n",
    "        label = nid_list.index(validation_file_to_nid[filename.replace(VAL_DIR, \"\")])\n",
    "        return Example(row.image, label)\n",
    "    features_df = image_df_filtered.rdd.map(map_label).toDF()\n",
    "    # Insert into feature store as a feature group\n",
    "    featurestore.create_featuregroup(features_df, VAL_FEATUREGROUP, \n",
    "                                     description = \"validation data for image classification with TinyImageNet, contains labelled images\",\n",
    "                                     feature_correlation=False, \n",
    "                                     cluster_analysis=False, feature_histograms=False,\n",
    "                                     descriptive_statistics=True, \n",
    "                                     featuregroup_version=VAL_FEATUREGROUP_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run The Pipeline\n",
    "\n",
    "1. Insert training data in the feature store as a feature group\n",
    "2. Insert test data in the feature store as a feature group\n",
    "3. Insert validation data in the feature store as a feature group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_train_dirs_into_featurestore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_test_dir_into_featurestore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_val_dir_into_featurestore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three feature groups in the feature store stored in ORC rather than a directory with tons of JPEGs and attached metadata. \n",
    "\n",
    "- train_tinyimagenet\n",
    "- test_tinyimagenet\n",
    "- val_tinyimagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify The Results\n",
    "\n",
    "TinyImageNet dataset contains a training set of 100,000 images, a validation set of 10,000 images, and a test set of also 10,000 images. These images are sourced from 200 different classes of objects. The images are downscaled from the original ImageNet’s dataset size of 256x256 to 64x64. \n",
    "\n",
    "2% of the images are grey-scaled and the rest are colored. We have filtered out the grey scaled images, so the counts should be slightly lower than 100,000, 10,000, and 10,000. The dimensions should be 64x64x3 since the images are colored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(TRAIN_FEATUREGROUP).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(VAL_FEATUREGROUP).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(TEST_FEATUREGROUP).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Datasets\n",
    "\n",
    "We can now take the feature groups in the feature store and convert them into training datasets in a suitable format. TFRecords is the recommended format when using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np_array(image):\n",
    "    \"\"\"\n",
    "    Converts a spark image representation to a numpy array\n",
    "    \n",
    "    Args:\n",
    "        :image: the image to convert\n",
    "    \n",
    "    Returns:\n",
    "            A multi-dimensional numpy array representing the image\n",
    "    \"\"\"\n",
    "    height = image.height\n",
    "    width = image.width\n",
    "    nChannels = image.nChannels\n",
    "    return np.ndarray(\n",
    "        shape=(height, width, nChannels),\n",
    "        dtype=np.uint8,\n",
    "        buffer=image.data,\n",
    "        strides=(width * nChannels, nChannels, 1)).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset():\n",
    "    \"\"\"\n",
    "    Creates a tfrecords dataset in the feature store, containing the train images and labels\n",
    "    \"\"\"\n",
    "    # Read images from feature store (ORC)\n",
    "    feature_df = featurestore.get_featuregroup(TRAIN_FEATUREGROUP)\n",
    "    # Convert the image representation to a flattened float array\n",
    "    def mapper(row):\n",
    "        Example = Row(\"image\", \"label\")\n",
    "        label = row.label # long label\n",
    "        image = to_np_array(row.image).flatten().tolist() #12288 length array with the image data (64x64x3)\n",
    "        return Example(image, label)\n",
    "    feature_df2 = feature_df.rdd.filter(filter_shapes).map(mapper).toDF()\n",
    "    # Save as a training dataset in TFRecords format in the feature store\n",
    "    featurestore.create_training_dataset(feature_df2, \n",
    "                                         TRAIN_DATASET,\n",
    "                                         description=\"training dataset for TinyImageNet Classification, Stored as TFRecords\",\n",
    "                                        data_format=\"tfrecords\",\n",
    "                                        descriptive_statistics=True,\n",
    "                                        training_dataset_version=1,\n",
    "                                        feature_correlation=False, \n",
    "                                        cluster_analysis=False, \n",
    "                                        feature_histograms=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset():\n",
    "    \"\"\"\n",
    "    Creates a tfrecords dataset in the feature store, containing the test images\n",
    "    \"\"\"    \n",
    "    # Read images from feature store (ORC)\n",
    "    feature_df = featurestore.get_featuregroup(TEST_FEATUREGROUP)\n",
    "    # Convert the image representation to a flattened float array\n",
    "    def mapper(row):\n",
    "        Example = Row(\"image\")\n",
    "        image = to_np_array(row.image).flatten().tolist() #12288 length array with the image data (64x64x3)\n",
    "        return Example(image)\n",
    "    feature_df2 = feature_df.rdd.filter(filter_shapes).map(mapper).toDF()\n",
    "    # Save as a dataset in TFRecords format in the feature store\n",
    "    featurestore.create_training_dataset(feature_df2, \n",
    "                                         TEST_DATASET,\n",
    "                                         description=\"test dataset for TinyImageNet Classification, Stored as TFRecords\",\n",
    "                                        data_format=\"tfrecords\",\n",
    "                                        descriptive_statistics=True,\n",
    "                                        training_dataset_version=1,\n",
    "                                        feature_correlation=False, \n",
    "                                        cluster_analysis=False, \n",
    "                                        feature_histograms=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_val_dataset():\n",
    "    \"\"\"\n",
    "    Creates a tfrecords dataset in the feature store, containing the validation images and labels\n",
    "    \"\"\"\n",
    "    # Read images from feature store (ORC)\n",
    "    feature_df = featurestore.get_featuregroup(VAL_FEATUREGROUP)\n",
    "    # Convert the image representation to a flattened float array\n",
    "    def mapper(row):\n",
    "        Example = Row(\"image\", \"label\")\n",
    "        label = row.label # long label\n",
    "        image = to_np_array(row.image).flatten().tolist() #12288 length array with the image data (64x64x3)\n",
    "        return Example(image, label)\n",
    "    feature_df2 = feature_df.rdd.filter(filter_shapes).map(mapper).toDF()\n",
    "    # Save as a training dataset in TFRecords format in the feature store\n",
    "    featurestore.create_training_dataset(feature_df2, \n",
    "                                         VAL_DATASET,\n",
    "                                         description=\"validation dataset for TinyImageNet Classification, Stored as TFRecords\",\n",
    "                                        data_format=\"tfrecords\",\n",
    "                                        descriptive_statistics=True,\n",
    "                                        training_dataset_version=1,\n",
    "                                        feature_correlation=False, \n",
    "                                        cluster_analysis=False, \n",
    "                                        feature_histograms=False,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "1. Insert training data in the feature store as a training dataset\n",
    "2. Insert test data in the feature store as a training dataset\n",
    "3. Insert validation data in the feature store as a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_val_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Training Dataset\n",
    "\n",
    "We can read the tfrecords from disk and very that we stored the data correctly and that the labels match. Once again, to do plotting we have to use %%local so that it will run the computation on the local machine where we have a display, rather than remotely in the spark driver in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_path(TRAIN_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_tf_record_schema(TRAIN_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import pydoop.hdfs as py_hdfs\n",
    "# We have to redfine this function so that we can use it in the %%local environment\n",
    "def parse_metadata():\n",
    "    \"\"\" \n",
    "    Parses the words.txt file into a map of label -> words and a list of ordered nids (index of nid = integer label).\n",
    "    Also parses the val_annotations.txt file into a map of (validation_file_name --> nid)\n",
    "    \"\"\"\n",
    "    # list all directories in the train set, the directory name is the \"nid\" and identifies the label\n",
    "    train_dirs = py_hdfs.ls(TRAIN_DIR)\n",
    "    \n",
    "    # remove the path except the nid\n",
    "    train_nid_list = list(map(lambda x: x.replace(TRAIN_DIR + \"/\", \"\"), train_dirs))\n",
    "    \n",
    "    # the number of nids equal then number of unique classes/labels\n",
    "    num_classes = len(train_nid_list)\n",
    "    \n",
    "    # read the words.txt file that contains lines of the form \"nid\\twords\"\n",
    "    with py_hdfs.open(ID_TO_CLASS_FILE, 'r') as f:\n",
    "        file_lines = f.read().decode(\"utf-8\").split(\"\\n\")\n",
    "    label_to_word = {}\n",
    "    \n",
    "    for l in file_lines:\n",
    "        # parse each line\n",
    "        wnid, word = l.split('\\t')\n",
    "        if wnid in train_nid_list:\n",
    "            # convert the nids into integer labels by using the position in the index\n",
    "            label = train_nid_list.index(wnid)\n",
    "            word = str(label) + \": \" + word\n",
    "            # save the mapping of integer label --> words\n",
    "            label_to_word[label] = word\n",
    "    \n",
    "    # read the val_annotations.txt file that contains lines of the form: \n",
    "    # \"validation_image\\tnid\\tx_pos\\ty_pos\\tw_pos\\th_pos\"\n",
    "    with py_hdfs.open(VAL_LABELS_FILE, 'r') as f:\n",
    "        file_lines = f.read().decode(\"utf-8\").split(\"\\n\")\n",
    "    validation_file_to_nid = {}\n",
    "    for l in file_lines:\n",
    "        # parse each line\n",
    "        tokens = l.split('\\t')\n",
    "        #skip corrupted lines\n",
    "        if len(tokens) > 2:\n",
    "            validation_img = tokens[0]\n",
    "            wnid = tokens[1]\n",
    "            # we only care about classification in this tutorial, not localization \n",
    "            if wnid in train_nid_list:\n",
    "                validation_file_to_nid[validation_img] = wnid\n",
    "    \n",
    "    return train_nid_list, label_to_word, validation_file_to_nid\n",
    "\n",
    "def create_tf_dataset():\n",
    "    \"\"\"\n",
    "    Creates a Tensorflow Dataset from TFRecords on HopsFS stored in the feature store\n",
    "    \"\"\"\n",
    "    dataset_dir = 'hdfs://10.0.104.196:8020/Projects/end_to_end/end_to_end_Training_Datasets/train_dataset_tinyimagenet_1/train_dataset_tinyimagenet'\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    tf_record_schema = {\n",
    "        'image': tf.FixedLenFeature(shape=[12288], dtype=tf.int64, default_value=None),\n",
    "        'label': tf.FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)\n",
    "    }\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        label = example[\"label\"]\n",
    "        image_flat = example[\"image\"]\n",
    "        image = tf.reshape(image_flat, (64,64,3))\n",
    "        return image, label\n",
    "\n",
    "    dataset = dataset.map(decode).shuffle(100)\n",
    "    return dataset\n",
    "\n",
    "# Read 8 images from HopsFS and decode the protobufs\n",
    "sample_images_parsed = []\n",
    "sample_labels_parsed = []\n",
    "train_nid_list, label_to_word, validation_file_to_nid = parse_metadata() \n",
    "with tf.Session() as sess:\n",
    "    dataset = create_tf_dataset()\n",
    "    dataset_iter = dataset.make_one_shot_iterator()\n",
    "    for i in range(8):\n",
    "        x,y = sess.run(dataset_iter.get_next())\n",
    "        sample_images_parsed.append(x)\n",
    "        sample_labels_parsed.append(y)\n",
    "\n",
    "# Plot the parsed images        \n",
    "plt.rcParams[\"figure.figsize\"] = (14,10)\n",
    "count = 0\n",
    "for i in range(len(sample_images_parsed)):\n",
    "    count += 1\n",
    "    plt.subplot(4,4,count)\n",
    "    plt.title('label: {}'.format(label_to_word[sample_labels_parsed[i]]))\n",
    "    plt.imshow(sample_images_parsed[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}